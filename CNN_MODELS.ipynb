{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tinotenda-29/TINOTENDA_MATIRAMBA_R229119Q_21_CNN_MODELS_ASSIGNMENT-1/blob/main/CNN_MODELS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l43N9RXScUOu",
        "outputId": "75e45dfa-0e4c-4917-dfab-4f473e52905c"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Configuration Complete. Image size: 128x128. Epochs: 8, LR: 0.01\n",
            "\n",
            "Loading CSV data and preparing tabular features...\n",
            "Tabular data shape after preprocessing: (1591, 104)\n",
            "\n",
            "âœ… Target variable 'price' has been Log-Transformed and Standard Scaled for maximum stability.\n",
            "Found image directory at /content/gdrive/MyDrive/images.\n",
            "\n",
            "Preparing image data with masking strategy...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading Images and Creating Masks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1591/1591 [00:15<00:00, 101.58it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total records processed: 1591. Records with images: 466\n",
            "Image data shape: (1591, 128, 128, 3). Mask shape: (1591,)\n",
            "\n",
            "Training samples: 1272, Test samples: 319\n",
            "\n",
            "Starting sequential training of the 20 CNN-backed models (Tuning: EPOCHS=8, LR=0.01, Loss=Huber, Architecture=Late Fusion)...\n",
            "\n",
            "================ Model 1/20: AlexNet ================\n",
            "Training AlexNet...\n",
            "Epoch 1/8\n",
            "40/40 - 59s - 1s/step - loss: 7.0722 - mae: 7.4195 - val_loss: 0.2924 - val_mae: 0.6106\n",
            "Epoch 2/8\n",
            "40/40 - 57s - 1s/step - loss: 0.3262 - mae: 0.6631 - val_loss: 0.2956 - val_mae: 0.6054\n",
            "Epoch 3/8\n",
            "40/40 - 81s - 2s/step - loss: 0.3129 - mae: 0.6381 - val_loss: 0.2996 - val_mae: 0.6337\n",
            "Epoch 4/8\n",
            "40/40 - 81s - 2s/step - loss: 0.3123 - mae: 0.6330 - val_loss: 0.2873 - val_mae: 0.6050\n",
            "Epoch 5/8\n",
            "40/40 - 82s - 2s/step - loss: 0.3088 - mae: 0.6305 - val_loss: 0.2856 - val_mae: 0.5934\n",
            "Epoch 6/8\n",
            "40/40 - 58s - 1s/step - loss: 0.3016 - mae: 0.6185 - val_loss: 0.2861 - val_mae: 0.5924\n",
            "Epoch 7/8\n",
            "40/40 - 80s - 2s/step - loss: 0.3045 - mae: 0.6204 - val_loss: 0.2853 - val_mae: 0.5933\n",
            "Epoch 8/8\n",
            "40/40 - 56s - 1s/step - loss: 0.2970 - mae: 0.6113 - val_loss: 0.2932 - val_mae: 0.6167\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 302ms/step\n",
            "Training Complete. | *RMSE: 4,428,327.46* | MAE: 664,932.82 | R2: -0.0027\n",
            "\n",
            "================ Model 2/20: NiN ================\n",
            "Training NiN...\n",
            "Epoch 1/8\n",
            "40/40 - 108s - 3s/step - loss: 0.3599 - mae: 0.7039 - val_loss: 0.3033 - val_mae: 0.6365\n",
            "Epoch 2/8\n",
            "40/40 - 99s - 2s/step - loss: 0.3282 - mae: 0.6611 - val_loss: 0.2925 - val_mae: 0.6102\n",
            "Epoch 3/8\n",
            "40/40 - 140s - 4s/step - loss: 0.3161 - mae: 0.6415 - val_loss: 0.3008 - val_mae: 0.6220\n",
            "Epoch 4/8\n",
            "40/40 - 143s - 4s/step - loss: 0.3199 - mae: 0.6442 - val_loss: 0.2928 - val_mae: 0.6021\n",
            "Epoch 5/8\n",
            "40/40 - 100s - 3s/step - loss: 0.2982 - mae: 0.6107 - val_loss: 0.2843 - val_mae: 0.5912\n",
            "Epoch 6/8\n",
            "40/40 - 100s - 2s/step - loss: 0.3075 - mae: 0.6227 - val_loss: 0.2905 - val_mae: 0.5948\n",
            "Epoch 7/8\n",
            "40/40 - 137s - 3s/step - loss: 0.2967 - mae: 0.6119 - val_loss: 0.3061 - val_mae: 0.6160\n",
            "Epoch 8/8\n",
            "40/40 - 100s - 2s/step - loss: 0.3005 - mae: 0.6146 - val_loss: 0.2874 - val_mae: 0.5955\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 620ms/step\n",
            "Training Complete. | *RMSE: 4,446,334.39* | MAE: 643,125.15 | R2: -0.0109\n",
            "\n",
            "================ Model 3/20: ZNet ================\n",
            "Training ZNet...\n",
            "Epoch 1/8\n",
            "40/40 - 54s - 1s/step - loss: 0.3986 - mae: 0.7453 - val_loss: 0.2863 - val_mae: 0.5981\n",
            "Epoch 2/8\n",
            "40/40 - 51s - 1s/step - loss: 0.3368 - mae: 0.6730 - val_loss: 0.3048 - val_mae: 0.6253\n",
            "Epoch 3/8\n",
            "40/40 - 81s - 2s/step - loss: 0.3313 - mae: 0.6624 - val_loss: 0.2923 - val_mae: 0.6027\n",
            "Epoch 4/8\n",
            "40/40 - 79s - 2s/step - loss: 0.3124 - mae: 0.6343 - val_loss: 0.2902 - val_mae: 0.6018\n",
            "Epoch 5/8\n",
            "40/40 - 47s - 1s/step - loss: 0.3087 - mae: 0.6309 - val_loss: 0.2951 - val_mae: 0.6026\n",
            "Epoch 6/8\n",
            "40/40 - 84s - 2s/step - loss: 0.3115 - mae: 0.6285 - val_loss: 0.2890 - val_mae: 0.6090\n",
            "Epoch 7/8\n",
            "40/40 - 49s - 1s/step - loss: 0.3002 - mae: 0.6120 - val_loss: 0.2986 - val_mae: 0.6175\n",
            "Epoch 8/8\n",
            "40/40 - 81s - 2s/step - loss: 0.3046 - mae: 0.6204 - val_loss: 0.2890 - val_mae: 0.6037\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 317ms/step\n",
            "Training Complete. | *RMSE: 4,447,975.91* | MAE: 646,543.01 | R2: -0.0117\n",
            "\n",
            "================ Model 4/20: VGG ================\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Training VGG...\n",
            "Epoch 1/8\n",
            "40/40 - 341s - 9s/step - loss: 0.3563 - mae: 0.7023 - val_loss: 0.2916 - val_mae: 0.6138\n",
            "Epoch 2/8\n",
            "40/40 - 334s - 8s/step - loss: 0.3179 - mae: 0.6495 - val_loss: 0.2877 - val_mae: 0.6032\n",
            "Epoch 3/8\n",
            "40/40 - 320s - 8s/step - loss: 0.3032 - mae: 0.6242 - val_loss: 0.2877 - val_mae: 0.6050\n",
            "Epoch 4/8\n",
            "40/40 - 335s - 8s/step - loss: 0.2948 - mae: 0.6127 - val_loss: 0.2861 - val_mae: 0.6014\n",
            "Epoch 5/8\n",
            "40/40 - 332s - 8s/step - loss: 0.2918 - mae: 0.6047 - val_loss: 0.2828 - val_mae: 0.6014\n",
            "Epoch 6/8\n",
            "40/40 - 313s - 8s/step - loss: 0.2848 - mae: 0.5984 - val_loss: 0.2813 - val_mae: 0.5988\n",
            "Epoch 7/8\n",
            "40/40 - 324s - 8s/step - loss: 0.2789 - mae: 0.5908 - val_loss: 0.2856 - val_mae: 0.5979\n",
            "Epoch 8/8\n",
            "40/40 - 316s - 8s/step - loss: 0.2839 - mae: 0.5955 - val_loss: 0.3050 - val_mae: 0.6361\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 6s/step\n",
            "Training Complete. | *RMSE: 4,462,083.37* | MAE: 660,338.47 | R2: -0.0181\n",
            "\n",
            "================ Model 5/20: Google LeNet ================\n",
            "Training Google LeNet...\n",
            "Epoch 1/8\n",
            "40/40 - 18s - 446ms/step - loss: 0.3490 - mae: 0.6984 - val_loss: 0.3017 - val_mae: 0.6330\n",
            "Epoch 2/8\n",
            "40/40 - 16s - 394ms/step - loss: 0.3273 - mae: 0.6602 - val_loss: 0.2958 - val_mae: 0.6091\n",
            "Epoch 3/8\n",
            "40/40 - 19s - 471ms/step - loss: 0.3184 - mae: 0.6458 - val_loss: 0.2928 - val_mae: 0.6139\n",
            "Epoch 4/8\n",
            "40/40 - 14s - 341ms/step - loss: 0.3160 - mae: 0.6379 - val_loss: 0.3001 - val_mae: 0.6109\n",
            "Epoch 5/8\n",
            "40/40 - 14s - 342ms/step - loss: 0.3106 - mae: 0.6270 - val_loss: 0.3008 - val_mae: 0.6233\n",
            "Epoch 6/8\n",
            "40/40 - 20s - 511ms/step - loss: 0.3104 - mae: 0.6328 - val_loss: 0.2981 - val_mae: 0.6138\n",
            "Epoch 7/8\n",
            "40/40 - 21s - 523ms/step - loss: 0.3133 - mae: 0.6359 - val_loss: 0.2913 - val_mae: 0.6085\n",
            "Epoch 8/8\n",
            "40/40 - 14s - 341ms/step - loss: 0.2984 - mae: 0.6136 - val_loss: 0.2927 - val_mae: 0.6023\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step\n",
            "Training Complete. | *RMSE: 4,450,241.42* | MAE: 645,556.38 | R2: -0.0127\n",
            "\n",
            "================ Model 6/20: InceptionV3 ================\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m87910968/87910968\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Training InceptionV3...\n",
            "Epoch 1/8\n",
            "40/40 - 66s - 2s/step - loss: 0.4789 - mae: 0.8367 - val_loss: 0.3034 - val_mae: 0.6420\n",
            "Epoch 2/8\n",
            "40/40 - 79s - 2s/step - loss: 0.3157 - mae: 0.6485 - val_loss: 0.3023 - val_mae: 0.6295\n",
            "Epoch 3/8\n",
            "40/40 - 50s - 1s/step - loss: 0.3133 - mae: 0.6369 - val_loss: 0.2967 - val_mae: 0.6212\n",
            "Epoch 4/8\n",
            "40/40 - 85s - 2s/step - loss: 0.2948 - mae: 0.6134 - val_loss: 0.3009 - val_mae: 0.6072\n",
            "Epoch 5/8\n",
            "40/40 - 78s - 2s/step - loss: 0.2795 - mae: 0.5857 - val_loss: 0.2973 - val_mae: 0.6189\n",
            "Epoch 6/8\n",
            "40/40 - 52s - 1s/step - loss: 0.2785 - mae: 0.5910 - val_loss: 0.2961 - val_mae: 0.6090\n",
            "Epoch 7/8\n",
            "40/40 - 60s - 1s/step - loss: 0.2602 - mae: 0.5602 - val_loss: 0.2910 - val_mae: 0.6102\n",
            "Epoch 8/8\n",
            "40/40 - 50s - 1s/step - loss: 0.2762 - mae: 0.5886 - val_loss: 0.3337 - val_mae: 0.6677\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1s/step\n",
            "Training Complete. | *RMSE: 4,528,829.42* | MAE: 772,067.17 | R2: -0.0488\n",
            "\n",
            "================ Model 7/20: Highway ================\n",
            "Training Highway...\n",
            "Epoch 1/8\n",
            "40/40 - 16s - 400ms/step - loss: 0.3632 - mae: 0.7120 - val_loss: 0.2950 - val_mae: 0.6145\n",
            "Epoch 2/8\n",
            "40/40 - 12s - 300ms/step - loss: 0.3253 - mae: 0.6544 - val_loss: 0.2877 - val_mae: 0.6025\n",
            "Epoch 3/8\n",
            "40/40 - 22s - 541ms/step - loss: 0.3312 - mae: 0.6603 - val_loss: 0.2938 - val_mae: 0.6158\n",
            "Epoch 4/8\n",
            "40/40 - 19s - 486ms/step - loss: 0.3190 - mae: 0.6392 - val_loss: 0.2928 - val_mae: 0.6086\n",
            "Epoch 5/8\n",
            "40/40 - 20s - 489ms/step - loss: 0.3092 - mae: 0.6251 - val_loss: 0.2937 - val_mae: 0.6004\n",
            "Epoch 6/8\n",
            "40/40 - 12s - 301ms/step - loss: 0.3156 - mae: 0.6398 - val_loss: 0.2945 - val_mae: 0.6146\n",
            "Epoch 7/8\n",
            "40/40 - 12s - 302ms/step - loss: 0.3043 - mae: 0.6216 - val_loss: 0.2924 - val_mae: 0.6078\n",
            "Epoch 8/8\n",
            "40/40 - 12s - 302ms/step - loss: 0.2979 - mae: 0.6105 - val_loss: 0.2932 - val_mae: 0.6116\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 195ms/step\n",
            "Training Complete. | *RMSE: 4,452,206.50* | MAE: 648,587.10 | R2: -0.0136\n",
            "\n",
            "================ Model 8/20: InceptionV4 ================\n",
            "Training InceptionV4...\n",
            "Epoch 1/8\n",
            "40/40 - 16s - 396ms/step - loss: 0.3456 - mae: 0.6848 - val_loss: 0.2899 - val_mae: 0.6039\n",
            "Epoch 2/8\n",
            "40/40 - 11s - 282ms/step - loss: 0.3464 - mae: 0.6831 - val_loss: 0.2920 - val_mae: 0.6141\n",
            "Epoch 3/8\n",
            "40/40 - 21s - 534ms/step - loss: 0.3262 - mae: 0.6526 - val_loss: 0.2902 - val_mae: 0.6104\n",
            "Epoch 4/8\n",
            "40/40 - 20s - 495ms/step - loss: 0.3071 - mae: 0.6286 - val_loss: 0.2976 - val_mae: 0.6083\n",
            "Epoch 5/8\n",
            "40/40 - 11s - 274ms/step - loss: 0.3159 - mae: 0.6392 - val_loss: 0.2850 - val_mae: 0.5961\n",
            "Epoch 6/8\n",
            "40/40 - 22s - 543ms/step - loss: 0.3036 - mae: 0.6184 - val_loss: 0.2849 - val_mae: 0.5968\n",
            "Epoch 7/8\n",
            "40/40 - 19s - 472ms/step - loss: 0.2977 - mae: 0.6100 - val_loss: 0.2819 - val_mae: 0.5902\n",
            "Epoch 8/8\n",
            "40/40 - 12s - 293ms/step - loss: 0.2953 - mae: 0.6066 - val_loss: 0.2874 - val_mae: 0.5967\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 131ms/step\n",
            "Training Complete. | *RMSE: 4,445,802.36* | MAE: 643,095.09 | R2: -0.0107\n",
            "\n",
            "================ Model 9/20: ResNet ================\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Training ResNet...\n",
            "Epoch 1/8\n",
            "40/40 - 108s - 3s/step - loss: 0.3550 - mae: 0.7021 - val_loss: 0.2981 - val_mae: 0.6253\n",
            "Epoch 2/8\n",
            "40/40 - 136s - 3s/step - loss: 0.3327 - mae: 0.6692 - val_loss: 0.2845 - val_mae: 0.5997\n",
            "Epoch 3/8\n",
            "40/40 - 93s - 2s/step - loss: 0.3100 - mae: 0.6344 - val_loss: 0.2937 - val_mae: 0.6069\n",
            "Epoch 4/8\n",
            "40/40 - 95s - 2s/step - loss: 0.3133 - mae: 0.6329 - val_loss: 0.2888 - val_mae: 0.6120\n",
            "Epoch 5/8\n",
            "40/40 - 142s - 4s/step - loss: 0.3105 - mae: 0.6346 - val_loss: 0.2949 - val_mae: 0.6141\n",
            "Epoch 6/8\n",
            "40/40 - 95s - 2s/step - loss: 0.2983 - mae: 0.6139 - val_loss: 0.2826 - val_mae: 0.5905\n",
            "Epoch 7/8\n",
            "40/40 - 95s - 2s/step - loss: 0.3113 - mae: 0.6302 - val_loss: 0.2980 - val_mae: 0.6219\n",
            "Epoch 8/8\n",
            "40/40 - 143s - 4s/step - loss: 0.3053 - mae: 0.6208 - val_loss: 0.2883 - val_mae: 0.5999\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2s/step\n",
            "Training Complete. | *RMSE: 4,450,953.37* | MAE: 644,876.27 | R2: -0.0130\n",
            "\n",
            "================ Model 10/20: InceptionV4 ResNet ================\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m219055592/219055592\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Training InceptionV4 ResNet...\n",
            "Epoch 1/8\n",
            "40/40 - 144s - 4s/step - loss: 0.4534 - mae: 0.8077 - val_loss: 0.3104 - val_mae: 0.6489\n",
            "Epoch 2/8\n",
            "40/40 - 137s - 3s/step - loss: 0.3298 - mae: 0.6626 - val_loss: 0.3019 - val_mae: 0.6420\n",
            "Epoch 3/8\n",
            "40/40 - 129s - 3s/step - loss: 0.3011 - mae: 0.6217 - val_loss: 0.2899 - val_mae: 0.5980\n",
            "Epoch 4/8\n",
            "40/40 - 131s - 3s/step - loss: 0.3036 - mae: 0.6216 - val_loss: 0.2890 - val_mae: 0.6079\n",
            "Epoch 5/8\n",
            "40/40 - 114s - 3s/step - loss: 0.2770 - mae: 0.5874 - val_loss: 0.2959 - val_mae: 0.6108\n",
            "Epoch 6/8\n",
            "40/40 - 130s - 3s/step - loss: 0.2770 - mae: 0.5812 - val_loss: 0.2856 - val_mae: 0.5920\n",
            "Epoch 7/8\n",
            "40/40 - 130s - 3s/step - loss: 0.2787 - mae: 0.5879 - val_loss: 0.2960 - val_mae: 0.6043\n",
            "Epoch 8/8\n",
            "40/40 - 127s - 3s/step - loss: 0.2763 - mae: 0.5812 - val_loss: 0.2941 - val_mae: 0.6011\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 3s/step\n",
            "Training Complete. | *RMSE: 4,450,808.12* | MAE: 645,879.47 | R2: -0.0129\n",
            "\n",
            "================ Model 11/20: FractalNet ================\n",
            "Training FractalNet...\n",
            "Epoch 1/8\n",
            "40/40 - 16s - 410ms/step - loss: 0.3504 - mae: 0.6945 - val_loss: 0.3008 - val_mae: 0.6337\n",
            "Epoch 2/8\n",
            "40/40 - 12s - 312ms/step - loss: 0.3322 - mae: 0.6679 - val_loss: 0.2830 - val_mae: 0.5948\n",
            "Epoch 3/8\n",
            "40/40 - 12s - 304ms/step - loss: 0.3174 - mae: 0.6428 - val_loss: 0.2875 - val_mae: 0.6054\n",
            "Epoch 4/8\n",
            "40/40 - 12s - 300ms/step - loss: 0.3148 - mae: 0.6390 - val_loss: 0.2990 - val_mae: 0.6026\n",
            "Epoch 5/8\n",
            "40/40 - 11s - 267ms/step - loss: 0.3072 - mae: 0.6251 - val_loss: 0.2898 - val_mae: 0.6010\n",
            "Epoch 6/8\n",
            "40/40 - 12s - 302ms/step - loss: 0.3043 - mae: 0.6198 - val_loss: 0.2883 - val_mae: 0.6023\n",
            "Epoch 7/8\n",
            "40/40 - 12s - 303ms/step - loss: 0.2967 - mae: 0.6113 - val_loss: 0.2887 - val_mae: 0.6051\n",
            "Epoch 8/8\n",
            "40/40 - 21s - 518ms/step - loss: 0.3044 - mae: 0.6198 - val_loss: 0.2849 - val_mae: 0.5897\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 130ms/step\n",
            "Training Complete. | *RMSE: 4,451,085.94* | MAE: 641,397.60 | R2: -0.0131\n",
            "\n",
            "================ Model 12/20: WideResNet ================\n",
            "Training WideResNet...\n",
            "Epoch 1/8\n",
            "40/40 - 16s - 403ms/step - loss: 0.3540 - mae: 0.6966 - val_loss: 0.3145 - val_mae: 0.6570\n",
            "Epoch 2/8\n",
            "40/40 - 19s - 473ms/step - loss: 0.3280 - mae: 0.6650 - val_loss: 0.2880 - val_mae: 0.5984\n",
            "Epoch 3/8\n",
            "40/40 - 22s - 538ms/step - loss: 0.3172 - mae: 0.6451 - val_loss: 0.2930 - val_mae: 0.6106\n",
            "Epoch 4/8\n",
            "40/40 - 12s - 307ms/step - loss: 0.3229 - mae: 0.6482 - val_loss: 0.2936 - val_mae: 0.6070\n",
            "Epoch 5/8\n",
            "40/40 - 12s - 304ms/step - loss: 0.3089 - mae: 0.6275 - val_loss: 0.2880 - val_mae: 0.5967\n",
            "Epoch 6/8\n",
            "40/40 - 11s - 280ms/step - loss: 0.3048 - mae: 0.6229 - val_loss: 0.2909 - val_mae: 0.6014\n",
            "Epoch 7/8\n",
            "40/40 - 22s - 539ms/step - loss: 0.3084 - mae: 0.6261 - val_loss: 0.2840 - val_mae: 0.5908\n",
            "Epoch 8/8\n",
            "40/40 - 22s - 551ms/step - loss: 0.3012 - mae: 0.6212 - val_loss: 0.2912 - val_mae: 0.6022\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 132ms/step\n",
            "Training Complete. | *RMSE: 4,447,209.49* | MAE: 643,794.53 | R2: -0.0113\n",
            "\n",
            "================ Model 13/20: Xception ================\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m83683744/83683744\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Training Xception...\n",
            "Epoch 1/8\n",
            "40/40 - 143s - 4s/step - loss: 0.3643 - mae: 0.7091 - val_loss: 0.2938 - val_mae: 0.6175\n",
            "Epoch 2/8\n",
            "40/40 - 120s - 3s/step - loss: 0.3137 - mae: 0.6409 - val_loss: 0.2907 - val_mae: 0.6062\n",
            "Epoch 3/8\n",
            "40/40 - 147s - 4s/step - loss: 0.2999 - mae: 0.6231 - val_loss: 0.3000 - val_mae: 0.6202\n",
            "Epoch 4/8\n",
            "40/40 - 157s - 4s/step - loss: 0.3013 - mae: 0.6227 - val_loss: 0.2995 - val_mae: 0.6296\n",
            "Epoch 5/8\n",
            "40/40 - 122s - 3s/step - loss: 0.2811 - mae: 0.5929 - val_loss: 0.2929 - val_mae: 0.6158\n",
            "Epoch 6/8\n",
            "40/40 - 135s - 3s/step - loss: 0.2778 - mae: 0.5904 - val_loss: 0.3065 - val_mae: 0.6306\n",
            "Epoch 7/8\n",
            "40/40 - 140s - 3s/step - loss: 0.2738 - mae: 0.5816 - val_loss: 0.2976 - val_mae: 0.6063\n",
            "Epoch 8/8\n",
            "40/40 - 133s - 3s/step - loss: 0.2719 - mae: 0.5788 - val_loss: 0.3085 - val_mae: 0.6162\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3s/step\n",
            "Training Complete. | *RMSE: 4,452,010.71* | MAE: 649,272.82 | R2: -0.0135\n",
            "\n",
            "================ Model 14/20: Residual Attention Neural Network ================\n",
            "Training Residual Attention Neural Network...\n",
            "Epoch 1/8\n",
            "40/40 - 16s - 404ms/step - loss: 0.3501 - mae: 0.6930 - val_loss: 0.2894 - val_mae: 0.6078\n",
            "Epoch 2/8\n",
            "40/40 - 12s - 306ms/step - loss: 0.3229 - mae: 0.6509 - val_loss: 0.3017 - val_mae: 0.6332\n",
            "Epoch 3/8\n",
            "40/40 - 12s - 303ms/step - loss: 0.3311 - mae: 0.6605 - val_loss: 0.2915 - val_mae: 0.6126\n",
            "Epoch 4/8\n",
            "40/40 - 20s - 493ms/step - loss: 0.3209 - mae: 0.6468 - val_loss: 0.2946 - val_mae: 0.6102\n",
            "Epoch 5/8\n",
            "40/40 - 21s - 533ms/step - loss: 0.3105 - mae: 0.6320 - val_loss: 0.2936 - val_mae: 0.6063\n",
            "Epoch 6/8\n",
            "40/40 - 13s - 334ms/step - loss: 0.3026 - mae: 0.6216 - val_loss: 0.2960 - val_mae: 0.6206\n",
            "Epoch 7/8\n",
            "40/40 - 12s - 312ms/step - loss: 0.2976 - mae: 0.6123 - val_loss: 0.2932 - val_mae: 0.6060\n",
            "Epoch 8/8\n",
            "40/40 - 12s - 294ms/step - loss: 0.2981 - mae: 0.6113 - val_loss: 0.2914 - val_mae: 0.6107\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 134ms/step\n",
            "Training Complete. | *RMSE: 4,440,883.64* | MAE: 649,518.54 | R2: -0.0084\n",
            "\n",
            "================ Model 15/20: Squeeze and Excitation Networks ================\n",
            "Training Squeeze and Excitation Networks...\n",
            "Epoch 1/8\n",
            "40/40 - 17s - 429ms/step - loss: 0.3511 - mae: 0.6953 - val_loss: 0.2957 - val_mae: 0.6142\n",
            "Epoch 2/8\n",
            "40/40 - 12s - 296ms/step - loss: 0.3295 - mae: 0.6636 - val_loss: 0.2900 - val_mae: 0.6050\n",
            "Epoch 3/8\n",
            "40/40 - 11s - 276ms/step - loss: 0.3185 - mae: 0.6459 - val_loss: 0.2878 - val_mae: 0.6008\n",
            "Epoch 4/8\n",
            "40/40 - 22s - 542ms/step - loss: 0.3183 - mae: 0.6450 - val_loss: 0.2890 - val_mae: 0.6036\n",
            "Epoch 5/8\n",
            "40/40 - 12s - 302ms/step - loss: 0.3067 - mae: 0.6272 - val_loss: 0.2949 - val_mae: 0.6068\n",
            "Epoch 6/8\n",
            "40/40 - 12s - 302ms/step - loss: 0.2981 - mae: 0.6118 - val_loss: 0.2897 - val_mae: 0.6076\n",
            "Epoch 7/8\n",
            "40/40 - 20s - 512ms/step - loss: 0.2995 - mae: 0.6123 - val_loss: 0.2826 - val_mae: 0.5935\n",
            "Epoch 8/8\n",
            "40/40 - 22s - 544ms/step - loss: 0.3002 - mae: 0.6138 - val_loss: 0.2943 - val_mae: 0.6167\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 132ms/step\n",
            "Training Complete. | *RMSE: 4,450,064.56* | MAE: 649,064.43 | R2: -0.0126\n",
            "\n",
            "================ Model 16/20: Competitive Squeeze and Excitation Network ================\n",
            "Training Competitive Squeeze and Excitation Network...\n",
            "Epoch 1/8\n",
            "40/40 - 17s - 419ms/step - loss: 0.3536 - mae: 0.7014 - val_loss: 0.3001 - val_mae: 0.6273\n",
            "Epoch 2/8\n",
            "40/40 - 20s - 507ms/step - loss: 0.3324 - mae: 0.6637 - val_loss: 0.2893 - val_mae: 0.6029\n",
            "Epoch 3/8\n",
            "40/40 - 12s - 303ms/step - loss: 0.3100 - mae: 0.6327 - val_loss: 0.3031 - val_mae: 0.6261\n",
            "Epoch 4/8\n",
            "40/40 - 19s - 483ms/step - loss: 0.3135 - mae: 0.6379 - val_loss: 0.2875 - val_mae: 0.5988\n",
            "Epoch 5/8\n",
            "40/40 - 24s - 588ms/step - loss: 0.3040 - mae: 0.6228 - val_loss: 0.2903 - val_mae: 0.6052\n",
            "Epoch 6/8\n",
            "40/40 - 12s - 307ms/step - loss: 0.3032 - mae: 0.6219 - val_loss: 0.3096 - val_mae: 0.6381\n",
            "Epoch 7/8\n",
            "40/40 - 13s - 315ms/step - loss: 0.2998 - mae: 0.6160 - val_loss: 0.2819 - val_mae: 0.5892\n",
            "Epoch 8/8\n",
            "40/40 - 11s - 274ms/step - loss: 0.2974 - mae: 0.6093 - val_loss: 0.2842 - val_mae: 0.5871\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 212ms/step\n",
            "Training Complete. | *RMSE: 4,445,725.12* | MAE: 639,823.01 | R2: -0.0106\n",
            "\n",
            "================ Model 17/20: DenseNet ================\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m29084464/29084464\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Training DenseNet...\n",
            "Epoch 1/8\n",
            "40/40 - 110s - 3s/step - loss: 0.4037 - mae: 0.7531 - val_loss: 0.3052 - val_mae: 0.6391\n",
            "Epoch 2/8\n",
            "40/40 - 133s - 3s/step - loss: 0.3212 - mae: 0.6526 - val_loss: 0.2972 - val_mae: 0.6320\n",
            "Epoch 3/8\n",
            "40/40 - 89s - 2s/step - loss: 0.3058 - mae: 0.6306 - val_loss: 0.2769 - val_mae: 0.5881\n",
            "Epoch 4/8\n",
            "40/40 - 88s - 2s/step - loss: 0.2985 - mae: 0.6194 - val_loss: 0.2921 - val_mae: 0.6168\n",
            "Epoch 5/8\n",
            "40/40 - 140s - 3s/step - loss: 0.2779 - mae: 0.5869 - val_loss: 0.2846 - val_mae: 0.6094\n",
            "Epoch 6/8\n",
            "40/40 - 145s - 4s/step - loss: 0.2851 - mae: 0.5925 - val_loss: 0.2848 - val_mae: 0.6113\n",
            "Epoch 7/8\n",
            "40/40 - 140s - 3s/step - loss: 0.2750 - mae: 0.5803 - val_loss: 0.2831 - val_mae: 0.6043\n",
            "Epoch 8/8\n",
            "40/40 - 86s - 2s/step - loss: 0.2616 - mae: 0.5619 - val_loss: 0.2837 - val_mae: 0.6011\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step\n",
            "Training Complete. | *RMSE: 4,457,370.04* | MAE: 668,768.72 | R2: -0.0159\n",
            "\n",
            "================ Model 18/20: MobileNet V2 ================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-596566861.py:297: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_cnn = BaseModel(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Training MobileNet V2...\n",
            "Epoch 1/8\n",
            "40/40 - 30s - 754ms/step - loss: 0.3766 - mae: 0.7282 - val_loss: 0.2974 - val_mae: 0.6359\n",
            "Epoch 2/8\n",
            "40/40 - 39s - 968ms/step - loss: 0.3131 - mae: 0.6368 - val_loss: 0.3030 - val_mae: 0.6250\n",
            "Epoch 3/8\n",
            "40/40 - 20s - 508ms/step - loss: 0.2989 - mae: 0.6193 - val_loss: 0.2849 - val_mae: 0.6022\n",
            "Epoch 4/8\n",
            "40/40 - 19s - 486ms/step - loss: 0.2873 - mae: 0.5991 - val_loss: 0.3076 - val_mae: 0.6351\n",
            "Epoch 5/8\n",
            "40/40 - 23s - 574ms/step - loss: 0.2664 - mae: 0.5705 - val_loss: 0.2898 - val_mae: 0.6023\n",
            "Epoch 6/8\n",
            "40/40 - 38s - 959ms/step - loss: 0.2690 - mae: 0.5724 - val_loss: 0.2995 - val_mae: 0.6145\n",
            "Epoch 7/8\n",
            "40/40 - 21s - 523ms/step - loss: 0.2570 - mae: 0.5543 - val_loss: 0.2994 - val_mae: 0.6191\n",
            "Epoch 8/8\n",
            "40/40 - 21s - 537ms/step - loss: 0.2570 - mae: 0.5536 - val_loss: 0.3165 - val_mae: 0.6325\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 489ms/step\n",
            "Training Complete. | *RMSE: 4,462,011.76* | MAE: 684,904.02 | R2: -0.0181\n",
            "\n",
            "================ Model 19/20: Capsulenet ================\n",
            "Training Capsulenet...\n",
            "Epoch 1/8\n",
            "40/40 - 16s - 401ms/step - loss: 0.3550 - mae: 0.7017 - val_loss: 0.3030 - val_mae: 0.6308\n",
            "Epoch 2/8\n",
            "40/40 - 20s - 498ms/step - loss: 0.3286 - mae: 0.6630 - val_loss: 0.2999 - val_mae: 0.6321\n",
            "Epoch 3/8\n",
            "40/40 - 21s - 517ms/step - loss: 0.3326 - mae: 0.6620 - val_loss: 0.2909 - val_mae: 0.5990\n",
            "Epoch 4/8\n",
            "40/40 - 12s - 302ms/step - loss: 0.3260 - mae: 0.6470 - val_loss: 0.2877 - val_mae: 0.6069\n",
            "Epoch 5/8\n",
            "40/40 - 12s - 304ms/step - loss: 0.3147 - mae: 0.6370 - val_loss: 0.2859 - val_mae: 0.5973\n",
            "Epoch 6/8\n",
            "40/40 - 12s - 302ms/step - loss: 0.3087 - mae: 0.6280 - val_loss: 0.2876 - val_mae: 0.6004\n",
            "Epoch 7/8\n",
            "40/40 - 12s - 293ms/step - loss: 0.2982 - mae: 0.6169 - val_loss: 0.2928 - val_mae: 0.6072\n",
            "Epoch 8/8\n",
            "40/40 - 11s - 275ms/step - loss: 0.2985 - mae: 0.6133 - val_loss: 0.2870 - val_mae: 0.5995\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 128ms/step\n",
            "Training Complete. | *RMSE: 4,447,752.03* | MAE: 644,303.75 | R2: -0.0116\n",
            "\n",
            "================ Model 20/20: HRNet V2 ================\n",
            "Training HRNet V2...\n",
            "Epoch 1/8\n",
            "40/40 - 16s - 411ms/step - loss: 0.3485 - mae: 0.6922 - val_loss: 0.3102 - val_mae: 0.6508\n",
            "Epoch 2/8\n",
            "40/40 - 11s - 273ms/step - loss: 0.3203 - mae: 0.6436 - val_loss: 0.2833 - val_mae: 0.5975\n",
            "Epoch 3/8\n",
            "40/40 - 12s - 304ms/step - loss: 0.3275 - mae: 0.6547 - val_loss: 0.3062 - val_mae: 0.6453\n",
            "Epoch 4/8\n",
            "40/40 - 12s - 305ms/step - loss: 0.3154 - mae: 0.6433 - val_loss: 0.2904 - val_mae: 0.6086\n",
            "Epoch 5/8\n",
            "40/40 - 20s - 509ms/step - loss: 0.3026 - mae: 0.6201 - val_loss: 0.2931 - val_mae: 0.6022\n",
            "Epoch 6/8\n",
            "40/40 - 11s - 263ms/step - loss: 0.3010 - mae: 0.6142 - val_loss: 0.2958 - val_mae: 0.6111\n",
            "Epoch 7/8\n",
            "40/40 - 22s - 554ms/step - loss: 0.3081 - mae: 0.6289 - val_loss: 0.2898 - val_mae: 0.6027\n",
            "Epoch 8/8\n",
            "40/40 - 12s - 305ms/step - loss: 0.2979 - mae: 0.6131 - val_loss: 0.2956 - val_mae: 0.6079\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 199ms/step\n",
            "Training Complete. | *RMSE: 4,457,725.03* | MAE: 649,095.30 | R2: -0.0161\n",
            "\n",
            "==================================================\n",
            "             MODEL PERFORMANCE SUMMARY\n",
            "==================================================\n",
            "| Model Name                                 |        RMSE |    MAE |          R2 | Notes      |\n",
            "|:-------------------------------------------|------------:|-------:|------------:|:-----------|\n",
            "| AlexNet                                    | 4.42833e+06 | 664933 | -0.00273813 | Successful |\n",
            "| Residual Attention Neural Network          | 4.44088e+06 | 649519 | -0.00843257 | Successful |\n",
            "| Competitive Squeeze and Excitation Network | 4.44573e+06 | 639823 | -0.0106326  | Successful |\n",
            "| InceptionV4                                | 4.4458e+06  | 643095 | -0.0106677  | Successful |\n",
            "| NiN                                        | 4.44633e+06 | 643125 | -0.0109096  | Successful |\n",
            "| WideResNet                                 | 4.44721e+06 | 643795 | -0.0113076  | Successful |\n",
            "| Capsulenet                                 | 4.44775e+06 | 644304 | -0.0115543  | Successful |\n",
            "| ZNet                                       | 4.44798e+06 | 646543 | -0.0116562  | Successful |\n",
            "| Squeeze and Excitation Networks            | 4.45006e+06 | 649064 | -0.0126065  | Successful |\n",
            "| Google LeNet                               | 4.45024e+06 | 645556 | -0.012687   | Successful |\n",
            "| InceptionV4 ResNet                         | 4.45081e+06 | 645879 | -0.0129449  | Successful |\n",
            "| ResNet                                     | 4.45095e+06 | 644876 | -0.013011   | Successful |\n",
            "| FractalNet                                 | 4.45109e+06 | 641398 | -0.0130713  | Successful |\n",
            "| Xception                                   | 4.45201e+06 | 649273 | -0.0134923  | Successful |\n",
            "| Highway                                    | 4.45221e+06 | 648587 | -0.0135815  | Successful |\n",
            "| DenseNet                                   | 4.45737e+06 | 668769 | -0.0159339  | Successful |\n",
            "| HRNet V2                                   | 4.45773e+06 | 649095 | -0.0160957  | Successful |\n",
            "| MobileNet V2                               | 4.46201e+06 | 684904 | -0.0180509  | Successful |\n",
            "| VGG                                        | 4.46208e+06 | 660338 | -0.0180836  | Successful |\n",
            "| InceptionV3                                | 4.52883e+06 | 772067 | -0.0487694  | Successful |\n",
            "==================================================\n",
            "ðŸ¥‡ Best Model: AlexNet (RMSE: 4,428,327.46)\n",
            "==================================================\n",
            "\n",
            "âœ… Saved: best_model_actual_vs_predicted.png\n",
            "âœ… Saved: all_model_rmse_comparison.png\n",
            "âœ… Saved: best_model_training_history.png\n"
          ]
        }
      ],
      "source": [
        "#%%\n",
        "# ==============================================================================\n",
        "# STEP 1: SETUP, CONFIGURATION, AND IMPORTS (REVISED FOR LATE FUSION)\n",
        "# ==============================================================================\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, applications, optimizers\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Code to Mount Google Drive (Run this first in Colab) ---\n",
        "drive.mount('/content/gdrive')\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "# --- Configuration ---\n",
        "# CRITICAL: Ensure DRIVE_PATH points to the directory containing\n",
        "# 'final_zimbabwe_property_listings_complete.csv' AND the 'images' folder.\n",
        "DRIVE_PATH = '/content/gdrive/MyDrive/'\n",
        "CSV_FILENAME = os.path.join(DRIVE_PATH, 'ZIMP.csv')\n",
        "IMAGE_FOLDER_NAME = 'images'\n",
        "# --- Hyperparameters (Tuning Applied) ---\n",
        "IMG_WIDTH, IMG_HEIGHT, CHANNELS = 128, 128, 3\n",
        "EPOCHS = 30 # Tuned value\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 5e-4 # Tuned value\n",
        "FINE_TUNE = False\n",
        "\n",
        "# List of CNN backbones to test\n",
        "CNN_BACKBONES = [\n",
        "    'AlexNet', 'NiN', 'ZNet', 'VGG', 'Google LeNet', 'InceptionV3',\n",
        "    'Highway', 'InceptionV4', 'ResNet', 'InceptionV4 ResNet',\n",
        "    'FractalNet', 'WideResNet', 'Xception', 'Residual Attention Neural Network',\n",
        "    'Squeeze and Excitation Networks', 'Competitive Squeeze and Excitation Network',\n",
        "    'DenseNet', 'MobileNet V2', 'Capsulenet', 'HRNet V2'\n",
        "]\n",
        "\n",
        "# Shared dense layer configurations\n",
        "TABULAR_DENSE_LAYERS = [128, 64]\n",
        "# PREDICTION_HEAD_LAYERS now acts as the prediction head layers for both branches\n",
        "PREDICTION_HEAD_LAYERS = [128, 64]\n",
        "\n",
        "print(f\"Configuration Complete. Image size: {IMG_WIDTH}x{IMG_HEIGHT}. Epochs: {EPOCHS}, LR: {LEARNING_RATE}\")\n",
        "# Ensure we are in the correct directory for file operations\n",
        "os.chdir(DRIVE_PATH)\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 2: DATA LOADING, PREPROCESSING, AND LOG SCALING\n",
        "# ==============================================================================\n",
        "\n",
        "print('\\nLoading CSV data and preparing tabular features...')\n",
        "df = pd.read_csv(CSV_FILENAME)\n",
        "\n",
        "IMAGE_FILENAME_COLUMN = 'image_filenames'\n",
        "\n",
        "# Drop rows where 'price' or the image filename is missing\n",
        "df.dropna(subset=['price', IMAGE_FILENAME_COLUMN], inplace=True)\n",
        "\n",
        "# Define features and target\n",
        "tabular_features = [\n",
        "    'bedrooms', 'bathrooms', 'land_area', 'building_area',\n",
        "    'property_type', 'location'\n",
        "]\n",
        "target = 'price'\n",
        "\n",
        "# Filter DataFrame and impute missing values\n",
        "df_filtered = df[tabular_features + [target, IMAGE_FILENAME_COLUMN]].copy()\n",
        "\n",
        "# Impute numerical features with the median\n",
        "for col in ['bedrooms', 'bathrooms', 'land_area', 'building_area']:\n",
        "    df_filtered.loc[:, col] = df_filtered[col].fillna(df_filtered[col].median())\n",
        "\n",
        "# Impute categorical features with 'Unknown'\n",
        "for col in ['property_type', 'location']:\n",
        "    df_filtered.loc[:, col] = df_filtered[col].fillna('Unknown')\n",
        "\n",
        "X = df_filtered[tabular_features]\n",
        "y = df_filtered[target].values\n",
        "image_filenames = df_filtered[IMAGE_FILENAME_COLUMN].values.astype(str)\n",
        "\n",
        "# --- Define Preprocessing Pipelines for Tabular Features (X) ---\n",
        "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "numerical_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
        "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Apply preprocessing to X\n",
        "X_tabular = preprocessor.fit_transform(X).toarray()\n",
        "tabular_input_shape = (X_tabular.shape[1],)\n",
        "print(f\"Tabular data shape after preprocessing: {X_tabular.shape}\")\n",
        "\n",
        "\n",
        "# --- CRITICAL FIX: LOG TRANSFORM THE TARGET VARIABLE (y) ---\n",
        "y_target_log = np.log1p(y)\n",
        "target_scaler = StandardScaler()\n",
        "y_target_scaled = target_scaler.fit_transform(y_target_log.reshape(-1, 1)).flatten()\n",
        "y_target = y_target_scaled\n",
        "\n",
        "print(\"\\nâœ… Target variable 'price' has been Log-Transformed and Standard Scaled for maximum stability.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 3: IMAGE AND MASK PREPARATION (REVISED FOR FULL DATASET)\n",
        "# ==============================================================================\n",
        "\n",
        "def load_and_preprocess_image_data_with_mask(filenames, drive_path=DRIVE_PATH, image_folder=IMAGE_FOLDER_NAME):\n",
        "    \"\"\"Loads images or a zero-image placeholder, and generates a binary mask.\"\"\"\n",
        "    image_dir = os.path.join(drive_path, image_folder)\n",
        "    images = []\n",
        "    image_masks = []\n",
        "\n",
        "    available_files = set()\n",
        "    try:\n",
        "        if not os.path.exists(image_dir):\n",
        "            raise FileNotFoundError(f\"Image directory not found at {image_dir}\")\n",
        "        available_files = set(os.listdir(image_dir))\n",
        "        print(f\"Found image directory at {image_dir}.\")\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"ERROR: {e}. Assuming no images are available. Check DRIVE_PATH and image_folder.\")\n",
        "\n",
        "    print(\"\\nPreparing image data with masking strategy...\")\n",
        "\n",
        "    for filename in tqdm(filenames, desc=\"Loading Images and Creating Masks\"):\n",
        "        path = filename\n",
        "\n",
        "        if path in available_files:\n",
        "            try:\n",
        "                # Load, resize, and normalize the actual image\n",
        "                img = Image.open(os.path.join(image_dir, path)).convert('RGB')\n",
        "                img = img.resize((IMG_WIDTH, IMG_HEIGHT))\n",
        "                img_arr = img_to_array(img).astype('float32') / 255.0\n",
        "                images.append(img_arr)\n",
        "                image_masks.append(1.0) # Mask is 1 (Image is present)\n",
        "            except Exception:\n",
        "                # Fallback if image exists but is corrupted\n",
        "                images.append(np.zeros((IMG_HEIGHT, IMG_WIDTH, CHANNELS), dtype=np.float32))\n",
        "                image_masks.append(0.0) # Treat as missing\n",
        "        else:\n",
        "            # Placeholder for missing images\n",
        "            images.append(np.zeros((IMG_HEIGHT, IMG_WIDTH, CHANNELS), dtype=np.float32))\n",
        "            image_masks.append(0.0) # Mask is 0 (Image is missing)\n",
        "\n",
        "    image_data = np.array(images)\n",
        "    image_masks = np.array(image_masks)\n",
        "    image_input_shape = image_data.shape[1:]\n",
        "\n",
        "    print(f\"Total records processed: {len(image_data)}. Records with images: {int(np.sum(image_masks))}\")\n",
        "    print(f\"Image data shape: {image_data.shape}. Mask shape: {image_masks.shape}\")\n",
        "\n",
        "    return image_data, image_input_shape, image_masks\n",
        "\n",
        "# Execute loading and masking\n",
        "image_data, image_input_shape, image_masks = load_and_preprocess_image_data_with_mask(image_filenames)\n",
        "\n",
        "# Perform the final Train/Test Split (including the mask array)\n",
        "X_tab_train, X_tab_test, X_img_train, X_img_test, y_train, y_test, X_mask_train, X_mask_test = train_test_split(\n",
        "    X_tabular, image_data, y_target, image_masks, test_size=0.2, random_state=42\n",
        ")\n",
        "print(f\"\\nTraining samples: {len(y_train)}, Test samples: {len(y_test)}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 4: CNN STUB AND MODEL BUILDER DEFINITIONS (REVISION: HUBER Loss + LATE FUSION)\n",
        "# ==============================================================================\n",
        "\n",
        "# --- CNN Stubs (Feature Extraction) ---\n",
        "\n",
        "def nin_block(x, num_channels, block_name):\n",
        "    x = layers.Conv2D(num_channels, (5, 5), activation='relu', padding='same', name=f'{block_name}_conv1')(x)\n",
        "    x = layers.Conv2D(num_channels, (1, 1), activation='relu', padding='same', name=f'{block_name}_conv2')(x)\n",
        "    x = layers.Conv2D(num_channels, (1, 1), activation='relu', padding='same', name=f'{block_name}_conv3')(x)\n",
        "    return x\n",
        "\n",
        "def nin_net(input_tensor, name='NiN'):\n",
        "    x = nin_block(input_tensor, 32, block_name=f'{name}_b1')\n",
        "    x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
        "    x = nin_block(x, 64, block_name=f'{name}_b2')\n",
        "    x = layers.GlobalAveragePooling2D(name=f'{name}_avg_pool')(x)\n",
        "    return x\n",
        "\n",
        "def alexnet_block(input_tensor, name='AlexNet'):\n",
        "    x = layers.Conv2D(96, (11, 11), strides=(4, 4), activation='relu', name=f'{name}_conv1')(input_tensor)\n",
        "    x = layers.MaxPooling2D((3, 3), strides=(2, 2), name=f'{name}_pool1')(x)\n",
        "    x = layers.Conv2D(256, (5, 5), activation='relu', padding='same', name=f'{name}_conv2')(x)\n",
        "    x = layers.MaxPooling2D((3, 3), strides=(2, 2), name=f'{name}_pool2')(x)\n",
        "    x = layers.Conv2D(384, (3, 3), activation='relu', padding='same', name=f'{name}_conv3')(x)\n",
        "    x = layers.Conv2D(384, (3, 3), activation='relu', padding='same', name=f'{name}_conv4')(x)\n",
        "    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same', name=f'{name}_conv5')(x)\n",
        "    x = layers.GlobalAveragePooling2D(name=f'{name}_avg_pool')(x)\n",
        "    return x\n",
        "\n",
        "# Placeholder stubs for other models\n",
        "def znet_stub(input_tensor):\n",
        "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_tensor)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    return x\n",
        "def googlenet_stub(input_tensor):\n",
        "    x = layers.Conv2D(64, (7, 7), strides=(2, 2), padding='same', activation='relu')(input_tensor)\n",
        "    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    return x\n",
        "def highway_stub(input_tensor):\n",
        "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_tensor)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    return x\n",
        "def inceptionv4_stub(input_tensor):\n",
        "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_tensor)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    return x\n",
        "def fractalnet_stub(input_tensor):\n",
        "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_tensor)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    return x\n",
        "def wideresnet_stub(input_tensor):\n",
        "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_tensor)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    return x\n",
        "def res_attention_stub(input_tensor):\n",
        "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_tensor)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    return x\n",
        "def senet_stub(input_tensor):\n",
        "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_tensor)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    return x\n",
        "def comp_senet_stub(input_tensor):\n",
        "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_tensor)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    return x\n",
        "def capsulenet_stub(input_tensor):\n",
        "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_tensor)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    return x\n",
        "def hrnet_stub(input_tensor):\n",
        "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_tensor)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "# --- Dual Input - Model Builder (LATE FUSION) ---\n",
        "def build_fusion_model_with_cnn(cnn_name, tab_shape, img_shape, fine_tune_flag, tab_units, prediction_units):\n",
        "    \"\"\"Creates a Dual Input - Late Prediction Fusion model with masking and Huber loss.\"\"\"\n",
        "\n",
        "    safe_cnn_name = cnn_name.replace(' ', '_').replace('-', '_')\n",
        "\n",
        "    # --- Branch 1: Tabular Feature Extraction (MLP) ---\n",
        "    tab_input = layers.Input(shape=tab_shape, name=\"tabular_input\")\n",
        "    x_tab = tab_input\n",
        "    for units in tab_units:\n",
        "        x_tab = layers.Dense(units, activation='relu')(x_tab)\n",
        "        x_tab = layers.Dropout(0.3)(x_tab)\n",
        "    # Final feature vector for tabular branch\n",
        "    tab_features = layers.Dense(64, activation='relu', name=\"tab_dense_out\")(x_tab)\n",
        "\n",
        "    # --- Branch 2: Image Feature Extraction (CNN) ---\n",
        "    img_input = layers.Input(shape=img_shape, name=\"image_input\")\n",
        "\n",
        "    # --- Branch 3: Mask Input (Scalar 0 or 1) ---\n",
        "    mask_input = layers.Input(shape=(1,), name=\"mask_input\")\n",
        "\n",
        "    # A. Keras Applications Mapping (Pre-trained models)\n",
        "    BACKBONE_MAP = {\n",
        "        'VGG': applications.VGG16,\n",
        "        'InceptionV3': applications.InceptionV3,\n",
        "        'ResNet': applications.ResNet50,\n",
        "        'InceptionV4 ResNet': applications.InceptionResNetV2,\n",
        "        'Xception': applications.Xception,\n",
        "        'DenseNet': applications.DenseNet121,\n",
        "        'MobileNet V2': applications.MobileNetV2\n",
        "    }\n",
        "\n",
        "    x_img = None\n",
        "    if cnn_name in BACKBONE_MAP:\n",
        "        BaseModel = BACKBONE_MAP[cnn_name]\n",
        "        base_cnn = BaseModel(\n",
        "            weights='imagenet', include_top=False, input_tensor=img_input\n",
        "        )\n",
        "        base_cnn.trainable = fine_tune_flag\n",
        "        x_img = base_cnn.output\n",
        "        x_img = layers.GlobalAveragePooling2D(name=f'{safe_cnn_name}_avg_pool')(x_img)\n",
        "\n",
        "    # B. Custom Stubs Mapping\n",
        "    elif cnn_name == 'AlexNet':\n",
        "        x_img = alexnet_block(img_input)\n",
        "    elif cnn_name == 'NiN':\n",
        "        x_img = nin_net(img_input)\n",
        "    elif cnn_name == 'ZNet':\n",
        "        x_img = znet_stub(img_input)\n",
        "    elif cnn_name == 'Google LeNet':\n",
        "        x_img = googlenet_stub(img_input)\n",
        "    elif cnn_name == 'Highway':\n",
        "        x_img = highway_stub(img_input)\n",
        "    elif cnn_name == 'InceptionV4':\n",
        "        x_img = inceptionv4_stub(img_input)\n",
        "    elif cnn_name == 'FractalNet':\n",
        "        x_img = fractalnet_stub(img_input)\n",
        "    elif cnn_name == 'WideResNet':\n",
        "        x_img = wideresnet_stub(img_input)\n",
        "    elif cnn_name == 'Residual Attention Neural Network':\n",
        "        x_img = res_attention_stub(img_input)\n",
        "    elif cnn_name == 'Squeeze and Excitation Networks':\n",
        "        x_img = senet_stub(img_input)\n",
        "    elif cnn_name == 'Competitive Squeeze and Excitation Network':\n",
        "        x_img = comp_senet_stub(img_input)\n",
        "    elif cnn_name == 'Capsulenet':\n",
        "        x_img = capsulenet_stub(img_input)\n",
        "    elif cnn_name == 'HRNet V2':\n",
        "        x_img = hrnet_stub(img_input)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown CNN backbone: {cnn_name}\")\n",
        "\n",
        "    # Feature vector for image branch\n",
        "    img_features = layers.Dense(64, activation='relu', name=\"img_dense_pre_mask\")(x_img)\n",
        "\n",
        "    # CRITICAL MASKING STEP: Disable image features if mask is 0\n",
        "    img_features_masked = layers.multiply([img_features, mask_input], name=\"masked_img_features\")\n",
        "\n",
        "    # ðŸ”¥ LATE PREDICTION FUSION ARCHITECTURE ðŸ”¥\n",
        "\n",
        "    # 1. Tabular Prediction Head\n",
        "    x_tab_pred = tab_features\n",
        "    for units in prediction_units:\n",
        "        x_tab_pred = layers.Dense(units, activation='relu')(x_tab_pred)\n",
        "    tab_output = layers.Dense(1, name=\"tabular_prediction_output\")(x_tab_pred)\n",
        "\n",
        "    # 2. Image Prediction Head\n",
        "    x_img_pred = img_features_masked\n",
        "    for units in prediction_units:\n",
        "        x_img_pred = layers.Dense(units, activation='relu')(x_img_pred)\n",
        "    img_output = layers.Dense(1, name=\"image_prediction_output\")(x_img_pred)\n",
        "\n",
        "    # 3. Hybridization/Averaging (combines the two individual predictions)\n",
        "    # The final prediction is the average of the two separate learning pathways.\n",
        "    final_output = layers.Average(name=\"final_hybrid_prediction\")([tab_output, img_output])\n",
        "\n",
        "    # Build and compile the model\n",
        "    model = models.Model(inputs=[tab_input, img_input, mask_input], outputs=final_output, name=safe_cnn_name)\n",
        "\n",
        "    optimizer = optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "\n",
        "    # Use Huber Loss for robustness\n",
        "    model.compile(loss=tf.keras.losses.Huber(delta=1.0), optimizer=optimizer, metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 5: TRAINING, PREDICTION, AND EVALUATION LOOP\n",
        "# ==============================================================================\n",
        "results = []\n",
        "best_model_name = None\n",
        "best_rmse = np.inf\n",
        "best_history = None\n",
        "best_predictions = None\n",
        "best_true_values = None\n",
        "\n",
        "print(f\"\\nStarting sequential training of the 20 CNN-backed models (Tuning: EPOCHS={EPOCHS}, LR={LEARNING_RATE}, Loss=Huber, Architecture=Late Fusion)...\")\n",
        "\n",
        "for i, cnn_name in enumerate(CNN_BACKBONES):\n",
        "    # Clear session to prevent memory leaks/corruption between models\n",
        "    K.clear_session()\n",
        "    print(f\"\\n================ Model {i+1}/20: {cnn_name} ================\")\n",
        "\n",
        "    try:\n",
        "        model = build_fusion_model_with_cnn(\n",
        "            cnn_name=cnn_name,\n",
        "            tab_shape=tabular_input_shape,\n",
        "            img_shape=image_input_shape,\n",
        "            fine_tune_flag=FINE_TUNE,\n",
        "            tab_units=TABULAR_DENSE_LAYERS,\n",
        "            prediction_units=PREDICTION_HEAD_LAYERS # Using prediction_units for the fusion head\n",
        "        )\n",
        "\n",
        "        # Define the training and validation data dictionaries (including mask)\n",
        "        train_data = {\n",
        "            \"tabular_input\": X_tab_train,\n",
        "            \"image_input\": X_img_train,\n",
        "            \"mask_input\": X_mask_train.reshape(-1, 1) # Mask must be reshaped to (N, 1)\n",
        "        }\n",
        "        test_data = {\n",
        "            \"tabular_input\": X_tab_test,\n",
        "            \"image_input\": X_img_test,\n",
        "            \"mask_input\": X_mask_test.reshape(-1, 1)  # Mask must be reshaped to (N, 1)\n",
        "        }\n",
        "\n",
        "        # 2. Train the Model\n",
        "        print(f\"Training {cnn_name}...\")\n",
        "        history = model.fit(\n",
        "            x=train_data,\n",
        "            y=y_train,\n",
        "            validation_data=(test_data, y_test),\n",
        "            epochs=EPOCHS,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            verbose=2\n",
        "        )\n",
        "\n",
        "        # 3. Predict on Test Set (Scaled Output)\n",
        "        y_pred_scaled = model.predict(test_data).flatten()\n",
        "\n",
        "        # 4. Inverse Transform Predictions and True Values (Log Scaling Inverse)\n",
        "        y_test_log = target_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
        "        y_pred_log = target_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "        # Step 4b: Inverse Log the values to get original currency (exp(x) - 1)\n",
        "        y_test_original = np.expm1(y_test_log)\n",
        "        y_pred_original = np.expm1(y_pred_log)\n",
        "\n",
        "        # Ensure prices are non-negative\n",
        "        y_test_original[y_test_original < 0] = 0\n",
        "        y_pred_original[y_pred_original < 0] = 0\n",
        "\n",
        "        # 5. Evaluate with Original Values\n",
        "        mae = mean_absolute_error(y_test_original, y_pred_original)\n",
        "        mse = mean_squared_error(y_test_original, y_pred_original)\n",
        "        rmse = np.sqrt(mse)\n",
        "        r2 = r2_score(y_test_original, y_pred_original)\n",
        "\n",
        "        print(f\"Training Complete. | *RMSE: {rmse:,.2f}* | MAE: {mae:,.2f} | R2: {r2:.4f}\")\n",
        "\n",
        "        # Store results and track the best model\n",
        "        results.append({\n",
        "            'Model Name': cnn_name,\n",
        "            'RMSE': rmse, 'MAE': mae, 'R2': r2, 'Notes': 'Successful'\n",
        "        })\n",
        "        if rmse < best_rmse:\n",
        "            best_rmse = rmse\n",
        "            best_model_name = cnn_name\n",
        "            best_history = history\n",
        "            best_predictions = y_pred_original\n",
        "            best_true_values = y_test_original\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error training {cnn_name}. Skipping. Error: {e}\")\n",
        "        results.append({\n",
        "            'Model Name': cnn_name, 'RMSE': np.nan, 'MAE': np.nan, 'R2': np.nan,\n",
        "            'Notes': f'Failed: {e}'\n",
        "        })\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 6: REPORTING AND VISUALIZATION\n",
        "# ==============================================================================\n",
        "\n",
        "if results:\n",
        "    results_df = pd.DataFrame(results).sort_values(by='RMSE', ascending=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"             MODEL PERFORMANCE SUMMARY\")\n",
        "    print(\"=\"*50)\n",
        "    print(results_df.to_markdown(index=False))\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Check if a best model was successfully trained\n",
        "    if best_model_name and not np.isnan(best_rmse):\n",
        "        print(f\"ðŸ¥‡ Best Model: {best_model_name} (RMSE: {best_rmse:,.2f})\")\n",
        "    else:\n",
        "        print(\"ðŸš¨ Warning: No models completed successfully or a new best model could not be determined.\")\n",
        "\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # 1. Plotting the performance of the best model (Actual vs. Predicted)\n",
        "    if best_predictions is not None:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(best_true_values, best_predictions, alpha=0.5)\n",
        "\n",
        "        min_val = min(best_true_values.min(), best_predictions.min())\n",
        "        max_val = max(best_true_values.max(), best_predictions.max())\n",
        "\n",
        "        plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
        "        plt.xlabel(\"Actual Price (USD)\")\n",
        "        plt.ylabel(\"Predicted Price (USD)\")\n",
        "        plt.title(f\"Actual vs. Predicted Prices for Best Model: {best_model_name}\")\n",
        "        plt.grid(True)\n",
        "        plt.savefig('best_model_actual_vs_predicted.png')\n",
        "        plt.close()\n",
        "        print(\"\\nâœ… Saved: best_model_actual_vs_predicted.png\")\n",
        "\n",
        "    # 2. Plotting all model RMSEs\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    valid_results = results_df.dropna(subset=['RMSE'])\n",
        "    plt.bar(valid_results['Model Name'], valid_results['RMSE'], color='skyblue')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.ylabel(\"Root Mean Squared Error (RMSE)\")\n",
        "    plt.title(\"Comparison of Model Performance (RMSE)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('all_model_rmse_comparison.png')\n",
        "    plt.close()\n",
        "    print(\"âœ… Saved: all_model_rmse_comparison.png\")\n",
        "\n",
        "    # 3. Plotting Training History (Loss) of the Best Model\n",
        "    if best_history is not None:\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        # Note: The loss here is the scaled Huber loss, but still shows learning trend\n",
        "        plt.plot(best_history.history['loss'], label='Train Loss (Scaled Huber)')\n",
        "        plt.plot(best_history.history['val_loss'], label='Validation Loss (Scaled Huber)')\n",
        "        plt.title(f'{best_model_name} Training History (Log-Scaled Huber Loss)')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss (Huber)')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig('best_model_training_history.png')\n",
        "        plt.close()\n",
        "        print(\"âœ… Saved: best_model_training_history.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of Model Performance\n",
        "\n",
        "The analysis summarizes the performance of 20 Hybrid Dual-Input Regression Models designed to predict property prices, ranked by the Root Mean Squared Error (RMSE).Key FindingsBest Performer: AlexNet achieved the lowest RMSE of 4,428,327.46.Ranking: The models clustered closely together, with the best (AlexNet) and the worst (InceptionV3, RMSE: 4,528,834.40) models showing little difference in error magnitude.Critical Implication: Negative $\\mathbf{R}^2$A major finding is that all 20 models exhibited negative $\\mathbf{R}^2$ scores, ranging from -0.0027 to -0.0488.A negative $\\text{R}^2$ means that every model performs worse than a simple baseline that just predicts the average price of all properties.This indicates that the models, despite using complex CNN architectures (like VGG, ResNet, etc.) alongside tabular data, failed to capture the underlying variance in the property prices.\n",
        "\n",
        "\n",
        "Conclusion\n",
        "\n",
        "\n",
        "While AlexNet is statistically the best-performing option in this test set, none of the models are suitable for deployment. The extremely high RMSE (in the millions) and the universally negative $\\text{R}^2$ scores suggest fundamental issues, likely related to:\n",
        "\n",
        "Limited Data Sample: The necessary cleaning steps, including dropping rows with missing prices or images, resulted in a dataset that was too small for effectively training complex, high-capacity hybrid models (like those using deep CNNs).\n",
        "\n",
        "Image Feature Contamination: The models incorrectly extracted features because many property images contained more than one house or had irrelevant background details. This meant the CNNs picked up features from the wrong structure, corrupting the visual signal and making the late fusion process unreliable"
      ],
      "metadata": {
        "id": "77gbIE2qxlac"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNu68saN+KQhiqx6aeNBqMo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}